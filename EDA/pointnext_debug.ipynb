{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/wlsgur4011/part_assembly/src/pointnext\n"
     ]
    }
   ],
   "source": [
    "cd /data/wlsgur4011/part_assembly/src/pointnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data/wlsgur4011/part_assembly/src/pointnext/examples/segmentation/\")\n",
    "import sys\n",
    "sys.argv = [\"examples/segmentation/main.py\", \"--cfg\", \"cfgs/part_assembly/pointnext-l.yaml\", \"--debug\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "launch mp with 1 GPUs, current rank: 0\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mdist_url: tcp://localhost:8888\n",
      "dist_backend: nccl\n",
      "multiprocessing_distributed: False\n",
      "ngpus_per_node: 1\n",
      "world_size: 1\n",
      "launcher: mp\n",
      "local_rank: 0\n",
      "use_gpu: True\n",
      "seed: 7469\n",
      "epoch: 0\n",
      "epochs: 100\n",
      "ignore_index: None\n",
      "val_fn: validate\n",
      "deterministic: False\n",
      "sync_bn: False\n",
      "criterion_args:\n",
      "  NAME: CrossEntropy\n",
      "  label_smoothing: 0.2\n",
      "use_mask: False\n",
      "grad_norm_clip: 10\n",
      "layer_decay: 0\n",
      "step_per_update: 1\n",
      "start_epoch: 1\n",
      "sched_on_epoch: True\n",
      "wandb:\n",
      "  use_wandb: False\n",
      "  project: PointNeXt-S3DIS\n",
      "  tags: ['part_assembly', 'train', 'pointnext-l', 'ngpus1', 'seed7469']\n",
      "  name: part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "use_amp: False\n",
      "use_voting: False\n",
      "val_freq: 1\n",
      "resume: False\n",
      "test: False\n",
      "finetune: False\n",
      "mode: train\n",
      "logname: None\n",
      "load_path: None\n",
      "print_freq: 50\n",
      "save_freq: -1\n",
      "root_dir: log/part_assembly\n",
      "pretrained_path: None\n",
      "datatransforms:\n",
      "  train: ['ChromaticAutoContrast', 'PointsToTensor', 'PointCloudScaling', 'PointCloudXYZAlign', 'PointCloudRotation', 'PointCloudJitter', 'ChromaticDropGPU', 'ChromaticNormalize']\n",
      "  val: ['PointsToTensor', 'PointCloudXYZAlign', 'ChromaticNormalize']\n",
      "  vote: ['ChromaticDropGPU']\n",
      "  kwargs:\n",
      "    color_drop: 0.2\n",
      "    gravity_dim: 2\n",
      "    scale: [0.9, 1.1]\n",
      "    angle: [0, 0, 1]\n",
      "    jitter_sigma: 0.005\n",
      "    jitter_clip: 0.02\n",
      "feature_keys: x,heights\n",
      "dataset:\n",
      "  common:\n",
      "    NAME: S3DIS_DatasetStage1\n",
      "    train_data_root: /data/wlsgur4011/DataCollection/BreakingBad/data_split/artifact.train.pth\n",
      "    val_data_root: /data/wlsgur4011/DataCollection/BreakingBad/data_split/artifact.val.pth\n",
      "    voxel_size: 0.04\n",
      "    overfit: 32\n",
      "  train:\n",
      "    split: train\n",
      "    voxel_max: 24000\n",
      "    loop: 1\n",
      "    presample: False\n",
      "  val:\n",
      "    split: val\n",
      "    voxel_max: None\n",
      "    presample: True\n",
      "  test:\n",
      "    split: test\n",
      "    voxel_max: None\n",
      "    presample: False\n",
      "num_classes: 2\n",
      "batch_size: 8\n",
      "val_batch_size: 1\n",
      "dataloader:\n",
      "  num_workers: 6\n",
      "cls_weighed_loss: False\n",
      "optimizer:\n",
      "  NAME: adamw\n",
      "  weight_decay: 0.0001\n",
      "sched: cosine\n",
      "warmup_epochs: 0\n",
      "min_lr: 1e-05\n",
      "lr: 0.01\n",
      "log_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "model:\n",
      "  NAME: BaseSeg\n",
      "  encoder_args:\n",
      "    NAME: PointNextEncoder\n",
      "    blocks: [1, 3, 5, 3, 3]\n",
      "    strides: [1, 4, 4, 4, 4]\n",
      "    sa_layers: 1\n",
      "    sa_use_res: False\n",
      "    width: 32\n",
      "    in_channels: 4\n",
      "    expansion: 4\n",
      "    radius: 0.1\n",
      "    nsample: 32\n",
      "    aggr_args:\n",
      "      feature_type: dp_fj\n",
      "      reduction: max\n",
      "    group_args:\n",
      "      NAME: ballquery\n",
      "      normalize_dp: True\n",
      "    conv_args:\n",
      "      order: conv-norm-act\n",
      "    act_args:\n",
      "      act: relu\n",
      "    norm_args:\n",
      "      norm: bn\n",
      "  decoder_args:\n",
      "    NAME: PointNextDecoder\n",
      "  cls_args:\n",
      "    NAME: SegHead\n",
      "    num_classes: 2\n",
      "    in_channels: None\n",
      "    norm_args:\n",
      "      norm: bn\n",
      "rank: 0\n",
      "distributed: False\n",
      "mp: False\n",
      "task_name: part_assembly\n",
      "cfg_basename: pointnext-l\n",
      "opts: \n",
      "is_training: True\n",
      "run_name: part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "run_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "exp_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "ckpt_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/checkpoint\n",
      "log_path: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq.log\n",
      "cfg_path: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/cfg.yaml\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mdist_url: tcp://localhost:8888\n",
      "dist_backend: nccl\n",
      "multiprocessing_distributed: False\n",
      "ngpus_per_node: 1\n",
      "world_size: 1\n",
      "launcher: mp\n",
      "local_rank: 0\n",
      "use_gpu: True\n",
      "seed: 7469\n",
      "epoch: 0\n",
      "epochs: 100\n",
      "ignore_index: None\n",
      "val_fn: validate\n",
      "deterministic: False\n",
      "sync_bn: False\n",
      "criterion_args:\n",
      "  NAME: CrossEntropy\n",
      "  label_smoothing: 0.2\n",
      "use_mask: False\n",
      "grad_norm_clip: 10\n",
      "layer_decay: 0\n",
      "step_per_update: 1\n",
      "start_epoch: 1\n",
      "sched_on_epoch: True\n",
      "wandb:\n",
      "  use_wandb: False\n",
      "  project: PointNeXt-S3DIS\n",
      "  tags: ['part_assembly', 'train', 'pointnext-l', 'ngpus1', 'seed7469']\n",
      "  name: part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "use_amp: False\n",
      "use_voting: False\n",
      "val_freq: 1\n",
      "resume: False\n",
      "test: False\n",
      "finetune: False\n",
      "mode: train\n",
      "logname: None\n",
      "load_path: None\n",
      "print_freq: 50\n",
      "save_freq: -1\n",
      "root_dir: log/part_assembly\n",
      "pretrained_path: None\n",
      "datatransforms:\n",
      "  train: ['ChromaticAutoContrast', 'PointsToTensor', 'PointCloudScaling', 'PointCloudXYZAlign', 'PointCloudRotation', 'PointCloudJitter', 'ChromaticDropGPU', 'ChromaticNormalize']\n",
      "  val: ['PointsToTensor', 'PointCloudXYZAlign', 'ChromaticNormalize']\n",
      "  vote: ['ChromaticDropGPU']\n",
      "  kwargs:\n",
      "    color_drop: 0.2\n",
      "    gravity_dim: 2\n",
      "    scale: [0.9, 1.1]\n",
      "    angle: [0, 0, 1]\n",
      "    jitter_sigma: 0.005\n",
      "    jitter_clip: 0.02\n",
      "feature_keys: x,heights\n",
      "dataset:\n",
      "  common:\n",
      "    NAME: S3DIS_DatasetStage1\n",
      "    train_data_root: /data/wlsgur4011/DataCollection/BreakingBad/data_split/artifact.train.pth\n",
      "    val_data_root: /data/wlsgur4011/DataCollection/BreakingBad/data_split/artifact.val.pth\n",
      "    voxel_size: 0.04\n",
      "    overfit: 32\n",
      "  train:\n",
      "    split: train\n",
      "    voxel_max: 24000\n",
      "    loop: 1\n",
      "    presample: False\n",
      "  val:\n",
      "    split: val\n",
      "    voxel_max: None\n",
      "    presample: True\n",
      "  test:\n",
      "    split: test\n",
      "    voxel_max: None\n",
      "    presample: False\n",
      "num_classes: 2\n",
      "batch_size: 8\n",
      "val_batch_size: 1\n",
      "dataloader:\n",
      "  num_workers: 6\n",
      "cls_weighed_loss: False\n",
      "optimizer:\n",
      "  NAME: adamw\n",
      "  weight_decay: 0.0001\n",
      "sched: cosine\n",
      "warmup_epochs: 0\n",
      "min_lr: 1e-05\n",
      "lr: 0.01\n",
      "log_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "model:\n",
      "  NAME: BaseSeg\n",
      "  encoder_args:\n",
      "    NAME: PointNextEncoder\n",
      "    blocks: [1, 3, 5, 3, 3]\n",
      "    strides: [1, 4, 4, 4, 4]\n",
      "    sa_layers: 1\n",
      "    sa_use_res: False\n",
      "    width: 32\n",
      "    in_channels: 4\n",
      "    expansion: 4\n",
      "    radius: 0.1\n",
      "    nsample: 32\n",
      "    aggr_args:\n",
      "      feature_type: dp_fj\n",
      "      reduction: max\n",
      "    group_args:\n",
      "      NAME: ballquery\n",
      "      normalize_dp: True\n",
      "    conv_args:\n",
      "      order: conv-norm-act\n",
      "    act_args:\n",
      "      act: relu\n",
      "    norm_args:\n",
      "      norm: bn\n",
      "  decoder_args:\n",
      "    NAME: PointNextDecoder\n",
      "  cls_args:\n",
      "    NAME: SegHead\n",
      "    num_classes: 2\n",
      "    in_channels: None\n",
      "    norm_args:\n",
      "      norm: bn\n",
      "rank: 0\n",
      "distributed: False\n",
      "mp: False\n",
      "task_name: part_assembly\n",
      "cfg_basename: pointnext-l\n",
      "opts: \n",
      "is_training: True\n",
      "run_name: part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "run_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "exp_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "ckpt_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/checkpoint\n",
      "log_path: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq.log\n",
      "cfg_path: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/cfg.yaml\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mdist_url: tcp://localhost:8888\n",
      "dist_backend: nccl\n",
      "multiprocessing_distributed: False\n",
      "ngpus_per_node: 1\n",
      "world_size: 1\n",
      "launcher: mp\n",
      "local_rank: 0\n",
      "use_gpu: True\n",
      "seed: 7469\n",
      "epoch: 0\n",
      "epochs: 100\n",
      "ignore_index: None\n",
      "val_fn: validate\n",
      "deterministic: False\n",
      "sync_bn: False\n",
      "criterion_args:\n",
      "  NAME: CrossEntropy\n",
      "  label_smoothing: 0.2\n",
      "use_mask: False\n",
      "grad_norm_clip: 10\n",
      "layer_decay: 0\n",
      "step_per_update: 1\n",
      "start_epoch: 1\n",
      "sched_on_epoch: True\n",
      "wandb:\n",
      "  use_wandb: False\n",
      "  project: PointNeXt-S3DIS\n",
      "  tags: ['part_assembly', 'train', 'pointnext-l', 'ngpus1', 'seed7469']\n",
      "  name: part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "use_amp: False\n",
      "use_voting: False\n",
      "val_freq: 1\n",
      "resume: False\n",
      "test: False\n",
      "finetune: False\n",
      "mode: train\n",
      "logname: None\n",
      "load_path: None\n",
      "print_freq: 50\n",
      "save_freq: -1\n",
      "root_dir: log/part_assembly\n",
      "pretrained_path: None\n",
      "datatransforms:\n",
      "  train: ['ChromaticAutoContrast', 'PointsToTensor', 'PointCloudScaling', 'PointCloudXYZAlign', 'PointCloudRotation', 'PointCloudJitter', 'ChromaticDropGPU', 'ChromaticNormalize']\n",
      "  val: ['PointsToTensor', 'PointCloudXYZAlign', 'ChromaticNormalize']\n",
      "  vote: ['ChromaticDropGPU']\n",
      "  kwargs:\n",
      "    color_drop: 0.2\n",
      "    gravity_dim: 2\n",
      "    scale: [0.9, 1.1]\n",
      "    angle: [0, 0, 1]\n",
      "    jitter_sigma: 0.005\n",
      "    jitter_clip: 0.02\n",
      "feature_keys: x,heights\n",
      "dataset:\n",
      "  common:\n",
      "    NAME: S3DIS_DatasetStage1\n",
      "    train_data_root: /data/wlsgur4011/DataCollection/BreakingBad/data_split/artifact.train.pth\n",
      "    val_data_root: /data/wlsgur4011/DataCollection/BreakingBad/data_split/artifact.val.pth\n",
      "    voxel_size: 0.04\n",
      "    overfit: 32\n",
      "  train:\n",
      "    split: train\n",
      "    voxel_max: 24000\n",
      "    loop: 1\n",
      "    presample: False\n",
      "  val:\n",
      "    split: val\n",
      "    voxel_max: None\n",
      "    presample: True\n",
      "  test:\n",
      "    split: test\n",
      "    voxel_max: None\n",
      "    presample: False\n",
      "num_classes: 2\n",
      "batch_size: 8\n",
      "val_batch_size: 1\n",
      "dataloader:\n",
      "  num_workers: 6\n",
      "cls_weighed_loss: False\n",
      "optimizer:\n",
      "  NAME: adamw\n",
      "  weight_decay: 0.0001\n",
      "sched: cosine\n",
      "warmup_epochs: 0\n",
      "min_lr: 1e-05\n",
      "lr: 0.01\n",
      "log_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "model:\n",
      "  NAME: BaseSeg\n",
      "  encoder_args:\n",
      "    NAME: PointNextEncoder\n",
      "    blocks: [1, 3, 5, 3, 3]\n",
      "    strides: [1, 4, 4, 4, 4]\n",
      "    sa_layers: 1\n",
      "    sa_use_res: False\n",
      "    width: 32\n",
      "    in_channels: 4\n",
      "    expansion: 4\n",
      "    radius: 0.1\n",
      "    nsample: 32\n",
      "    aggr_args:\n",
      "      feature_type: dp_fj\n",
      "      reduction: max\n",
      "    group_args:\n",
      "      NAME: ballquery\n",
      "      normalize_dp: True\n",
      "    conv_args:\n",
      "      order: conv-norm-act\n",
      "    act_args:\n",
      "      act: relu\n",
      "    norm_args:\n",
      "      norm: bn\n",
      "  decoder_args:\n",
      "    NAME: PointNextDecoder\n",
      "  cls_args:\n",
      "    NAME: SegHead\n",
      "    num_classes: 2\n",
      "    in_channels: None\n",
      "    norm_args:\n",
      "      norm: bn\n",
      "rank: 0\n",
      "distributed: False\n",
      "mp: False\n",
      "task_name: part_assembly\n",
      "cfg_basename: pointnext-l\n",
      "opts: \n",
      "is_training: True\n",
      "run_name: part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "run_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "exp_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "ckpt_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/checkpoint\n",
      "log_path: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq.log\n",
      "cfg_path: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/cfg.yaml\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mdist_url: tcp://localhost:8888\n",
      "dist_backend: nccl\n",
      "multiprocessing_distributed: False\n",
      "ngpus_per_node: 1\n",
      "world_size: 1\n",
      "launcher: mp\n",
      "local_rank: 0\n",
      "use_gpu: True\n",
      "seed: 7469\n",
      "epoch: 0\n",
      "epochs: 100\n",
      "ignore_index: None\n",
      "val_fn: validate\n",
      "deterministic: False\n",
      "sync_bn: False\n",
      "criterion_args:\n",
      "  NAME: CrossEntropy\n",
      "  label_smoothing: 0.2\n",
      "use_mask: False\n",
      "grad_norm_clip: 10\n",
      "layer_decay: 0\n",
      "step_per_update: 1\n",
      "start_epoch: 1\n",
      "sched_on_epoch: True\n",
      "wandb:\n",
      "  use_wandb: False\n",
      "  project: PointNeXt-S3DIS\n",
      "  tags: ['part_assembly', 'train', 'pointnext-l', 'ngpus1', 'seed7469']\n",
      "  name: part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "use_amp: False\n",
      "use_voting: False\n",
      "val_freq: 1\n",
      "resume: False\n",
      "test: False\n",
      "finetune: False\n",
      "mode: train\n",
      "logname: None\n",
      "load_path: None\n",
      "print_freq: 50\n",
      "save_freq: -1\n",
      "root_dir: log/part_assembly\n",
      "pretrained_path: None\n",
      "datatransforms:\n",
      "  train: ['ChromaticAutoContrast', 'PointsToTensor', 'PointCloudScaling', 'PointCloudXYZAlign', 'PointCloudRotation', 'PointCloudJitter', 'ChromaticDropGPU', 'ChromaticNormalize']\n",
      "  val: ['PointsToTensor', 'PointCloudXYZAlign', 'ChromaticNormalize']\n",
      "  vote: ['ChromaticDropGPU']\n",
      "  kwargs:\n",
      "    color_drop: 0.2\n",
      "    gravity_dim: 2\n",
      "    scale: [0.9, 1.1]\n",
      "    angle: [0, 0, 1]\n",
      "    jitter_sigma: 0.005\n",
      "    jitter_clip: 0.02\n",
      "feature_keys: x,heights\n",
      "dataset:\n",
      "  common:\n",
      "    NAME: S3DIS_DatasetStage1\n",
      "    train_data_root: /data/wlsgur4011/DataCollection/BreakingBad/data_split/artifact.train.pth\n",
      "    val_data_root: /data/wlsgur4011/DataCollection/BreakingBad/data_split/artifact.val.pth\n",
      "    voxel_size: 0.04\n",
      "    overfit: 32\n",
      "  train:\n",
      "    split: train\n",
      "    voxel_max: 24000\n",
      "    loop: 1\n",
      "    presample: False\n",
      "  val:\n",
      "    split: val\n",
      "    voxel_max: None\n",
      "    presample: True\n",
      "  test:\n",
      "    split: test\n",
      "    voxel_max: None\n",
      "    presample: False\n",
      "num_classes: 2\n",
      "batch_size: 8\n",
      "val_batch_size: 1\n",
      "dataloader:\n",
      "  num_workers: 6\n",
      "cls_weighed_loss: False\n",
      "optimizer:\n",
      "  NAME: adamw\n",
      "  weight_decay: 0.0001\n",
      "sched: cosine\n",
      "warmup_epochs: 0\n",
      "min_lr: 1e-05\n",
      "lr: 0.01\n",
      "log_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "model:\n",
      "  NAME: BaseSeg\n",
      "  encoder_args:\n",
      "    NAME: PointNextEncoder\n",
      "    blocks: [1, 3, 5, 3, 3]\n",
      "    strides: [1, 4, 4, 4, 4]\n",
      "    sa_layers: 1\n",
      "    sa_use_res: False\n",
      "    width: 32\n",
      "    in_channels: 4\n",
      "    expansion: 4\n",
      "    radius: 0.1\n",
      "    nsample: 32\n",
      "    aggr_args:\n",
      "      feature_type: dp_fj\n",
      "      reduction: max\n",
      "    group_args:\n",
      "      NAME: ballquery\n",
      "      normalize_dp: True\n",
      "    conv_args:\n",
      "      order: conv-norm-act\n",
      "    act_args:\n",
      "      act: relu\n",
      "    norm_args:\n",
      "      norm: bn\n",
      "  decoder_args:\n",
      "    NAME: PointNextDecoder\n",
      "  cls_args:\n",
      "    NAME: SegHead\n",
      "    num_classes: 2\n",
      "    in_channels: None\n",
      "    norm_args:\n",
      "      norm: bn\n",
      "rank: 0\n",
      "distributed: False\n",
      "mp: False\n",
      "task_name: part_assembly\n",
      "cfg_basename: pointnext-l\n",
      "opts: \n",
      "is_training: True\n",
      "run_name: part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "run_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "exp_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq\n",
      "ckpt_dir: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/checkpoint\n",
      "log_path: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq.log\n",
      "cfg_path: log/part_assembly/part_assembly-train-pointnext-l-ngpus1-seed7469-20230524-163712-TgQ4spHxQhpqSix8g3UwVq/cfg.yaml\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mradius: [[0.1], [0.1, 0.2, 0.2], [0.2, 0.4, 0.4, 0.4, 0.4], [0.4, 0.8, 0.8], [0.8, 1.6, 1.6]],\n",
      " nsample: [[32], [32, 32, 32], [32, 32, 32, 32, 32], [32, 32, 32], [32, 32, 32]]\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mradius: [[0.1], [0.1, 0.2, 0.2], [0.2, 0.4, 0.4, 0.4, 0.4], [0.4, 0.8, 0.8], [0.8, 1.6, 1.6]],\n",
      " nsample: [[32], [32, 32, 32], [32, 32, 32, 32, 32], [32, 32, 32], [32, 32, 32]]\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mradius: [[0.1], [0.1, 0.2, 0.2], [0.2, 0.4, 0.4, 0.4, 0.4], [0.4, 0.8, 0.8], [0.8, 1.6, 1.6]],\n",
      " nsample: [[32], [32, 32, 32], [32, 32, 32, 32, 32], [32, 32, 32], [32, 32, 32]]\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mradius: [[0.1], [0.1, 0.2, 0.2], [0.2, 0.4, 0.4, 0.4, 0.4], [0.4, 0.8, 0.8], [0.8, 1.6, 1.6]],\n",
      " nsample: [[32], [32, 32, 32], [32, 32, 32, 32, 32], [32, 32, 32], [32, 32, 32]]\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.1\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.1\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.1\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.1\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.2\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.4\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 0.8\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 1.6\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 1.6\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 1.6\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 1.6\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 1.6\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 1.6\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 1.6\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNAME: ballquery\n",
      "normalize_dp: True\n",
      "radius: 1.6\n",
      "nsample: 32\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mBaseSeg(\n",
      "  (encoder): PointNextEncoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(4, 32, kernel_size=(1,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(35, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(67, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(131, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(259, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(515, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(515, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): PointNextDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(96, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(192, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(768, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): SegHead(\n",
      "    (head): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Sequential(\n",
      "        (0): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mBaseSeg(\n",
      "  (encoder): PointNextEncoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(4, 32, kernel_size=(1,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(35, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(67, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(131, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(259, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(515, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(515, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): PointNextDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(96, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(192, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(768, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): SegHead(\n",
      "    (head): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Sequential(\n",
      "        (0): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mBaseSeg(\n",
      "  (encoder): PointNextEncoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(4, 32, kernel_size=(1,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(35, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(67, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(131, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(259, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(515, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(515, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): PointNextDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(96, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(192, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(768, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): SegHead(\n",
      "    (head): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Sequential(\n",
      "        (0): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mBaseSeg(\n",
      "  (encoder): PointNextEncoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(4, 32, kernel_size=(1,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(35, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(67, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(131, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): SetAbstraction(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv2d(259, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (grouper): QueryAndGroup()\n",
      "        )\n",
      "        (1): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(515, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): InvResMLP(\n",
      "          (convs): LocalAggregation(\n",
      "            (convs): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2d(515, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (grouper): QueryAndGroup()\n",
      "          )\n",
      "          (pwconv): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): PointNextDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(96, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(192, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): FeaturePropogation(\n",
      "          (convs): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv1d(768, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): SegHead(\n",
      "    (head): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Sequential(\n",
      "        (0): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNumber of params: 7.1247 M\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNumber of params: 7.1247 M\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNumber of params: 7.1247 M\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mNumber of params: 7.1247 M\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mParam groups = {\n",
      "  \"decay\": {\n",
      "    \"weight_decay\": 0.0001,\n",
      "    \"params\": [\n",
      "      \"encoder.encoder.0.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.3.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.4.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.0.weight\",\n",
      "      \"decoder.decoder.0.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.0.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.1.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.1.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.2.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.2.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.3.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.3.0.convs.1.0.weight\",\n",
      "      \"head.head.0.0.weight\",\n",
      "      \"head.head.2.0.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  },\n",
      "  \"no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"encoder.encoder.0.0.convs.0.0.bias\",\n",
      "      \"encoder.encoder.1.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.3.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.4.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.1.bias\",\n",
      "      \"decoder.decoder.0.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.0.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.0.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.0.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.1.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.1.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.1.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.1.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.2.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.2.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.2.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.2.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.3.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.3.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.3.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.3.0.convs.1.1.bias\",\n",
      "      \"head.head.0.1.weight\",\n",
      "      \"head.head.0.1.bias\",\n",
      "      \"head.head.2.0.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  }\n",
      "}\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mParam groups = {\n",
      "  \"decay\": {\n",
      "    \"weight_decay\": 0.0001,\n",
      "    \"params\": [\n",
      "      \"encoder.encoder.0.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.3.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.4.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.0.weight\",\n",
      "      \"decoder.decoder.0.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.0.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.1.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.1.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.2.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.2.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.3.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.3.0.convs.1.0.weight\",\n",
      "      \"head.head.0.0.weight\",\n",
      "      \"head.head.2.0.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  },\n",
      "  \"no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"encoder.encoder.0.0.convs.0.0.bias\",\n",
      "      \"encoder.encoder.1.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.3.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.4.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.1.bias\",\n",
      "      \"decoder.decoder.0.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.0.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.0.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.0.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.1.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.1.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.1.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.1.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.2.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.2.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.2.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.2.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.3.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.3.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.3.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.3.0.convs.1.1.bias\",\n",
      "      \"head.head.0.1.weight\",\n",
      "      \"head.head.0.1.bias\",\n",
      "      \"head.head.2.0.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  }\n",
      "}\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mParam groups = {\n",
      "  \"decay\": {\n",
      "    \"weight_decay\": 0.0001,\n",
      "    \"params\": [\n",
      "      \"encoder.encoder.0.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.3.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.4.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.0.weight\",\n",
      "      \"decoder.decoder.0.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.0.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.1.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.1.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.2.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.2.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.3.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.3.0.convs.1.0.weight\",\n",
      "      \"head.head.0.0.weight\",\n",
      "      \"head.head.2.0.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  },\n",
      "  \"no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"encoder.encoder.0.0.convs.0.0.bias\",\n",
      "      \"encoder.encoder.1.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.3.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.4.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.1.bias\",\n",
      "      \"decoder.decoder.0.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.0.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.0.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.0.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.1.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.1.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.1.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.1.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.2.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.2.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.2.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.2.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.3.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.3.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.3.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.3.0.convs.1.1.bias\",\n",
      "      \"head.head.0.1.weight\",\n",
      "      \"head.head.0.1.bias\",\n",
      "      \"head.head.2.0.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  }\n",
      "}\n",
      "\u001b[32m[05/24 16:37:12 S3DIS_DatasetStage1]: \u001b[0mParam groups = {\n",
      "  \"decay\": {\n",
      "    \"weight_decay\": 0.0001,\n",
      "    \"params\": [\n",
      "      \"encoder.encoder.0.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.3.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.4.0.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.0.weight\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.0.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.0.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.0.weight\",\n",
      "      \"decoder.decoder.0.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.0.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.1.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.1.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.2.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.2.0.convs.1.0.weight\",\n",
      "      \"decoder.decoder.3.0.convs.0.0.weight\",\n",
      "      \"decoder.decoder.3.0.convs.1.0.weight\",\n",
      "      \"head.head.0.0.weight\",\n",
      "      \"head.head.2.0.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  },\n",
      "  \"no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"encoder.encoder.0.0.convs.0.0.bias\",\n",
      "      \"encoder.encoder.1.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.1.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.1.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.1.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.3.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.3.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.2.4.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.2.4.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.3.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.3.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.3.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.3.2.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.4.0.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.0.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.1.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.4.1.pwconv.1.1.bias\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.1.weight\",\n",
      "      \"encoder.encoder.4.2.convs.convs.0.1.bias\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.1.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.0.1.bias\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.1.weight\",\n",
      "      \"encoder.encoder.4.2.pwconv.1.1.bias\",\n",
      "      \"decoder.decoder.0.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.0.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.0.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.0.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.1.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.1.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.1.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.1.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.2.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.2.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.2.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.2.0.convs.1.1.bias\",\n",
      "      \"decoder.decoder.3.0.convs.0.1.weight\",\n",
      "      \"decoder.decoder.3.0.convs.0.1.bias\",\n",
      "      \"decoder.decoder.3.0.convs.1.1.weight\",\n",
      "      \"decoder.decoder.3.0.convs.1.1.bias\",\n",
      "      \"head.head.0.1.weight\",\n",
      "      \"head.head.0.1.bias\",\n",
      "      \"head.head.2.0.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  }\n",
      "}\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0m\n",
      "Totally 32 samples in val set\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0m\n",
      "Totally 32 samples in val set\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0m\n",
      "Totally 32 samples in val set\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0m\n",
      "Totally 32 samples in val set\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mlength of validation dataset: 32\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mlength of validation dataset: 32\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mlength of validation dataset: 32\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mlength of validation dataset: 32\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mnumber of classes of the dataset: 2\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mnumber of classes of the dataset: 2\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mnumber of classes of the dataset: 2\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mnumber of classes of the dataset: 2\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mTraining from scratch\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mTraining from scratch\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mTraining from scratch\n",
      "\u001b[32m[05/24 16:37:26 S3DIS_DatasetStage1]: \u001b[0mTraining from scratch\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0m\n",
      "Totally 32 samples in train set\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0m\n",
      "Totally 32 samples in train set\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0m\n",
      "Totally 32 samples in train set\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0m\n",
      "Totally 32 samples in train set\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0mlength of training dataset: 32\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0mlength of training dataset: 32\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0mlength of training dataset: 32\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0mlength of training dataset: 32\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0mnumber of training batch: 4\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0mnumber of training batch: 4\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0mnumber of training batch: 4\n",
      "\u001b[32m[05/24 16:37:45 S3DIS_DatasetStage1]: \u001b[0mnumber of training batch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   3%|▎         | 1/32 [00:00<00:04,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m3333  {\n",
      "    \"pos\": \"tensor[1, 207, 3] n=621 (2.4Kb) x∈[-0.274, 0.433] μ=0.070 σ=0.145 cuda:0\",\n",
      "    \"x\": \"tensor[1, 4, 207] n=828 (3.2Kb) x∈[-8.198, 2.722] μ=-1.884 σ=2.972 cuda:0\",\n",
      "    \"y\": \"tensor[1, 207] i64 1.6Kb x∈[0, 1] μ=0.836 σ=0.371 cuda:0\",\n",
      "    \"heights\": \"tensor[1, 207, 1] x∈[0.008, 0.441] μ=0.218 σ=0.116 cuda:0\"\n",
      "}\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "(Distributed) training script for scene segmentation\n",
    "This file currently supports training and testing on S3DIS\n",
    "If more than 1 GPU is provided, will launch multi processing distributed training by default\n",
    "if you only wana use 1 GPU, set `CUDA_VISIBLE_DEVICES` accordingly\n",
    "\"\"\"\n",
    "# import __init__\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import csv\n",
    "import wandb\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import distributed as dist, multiprocessing as mp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_scatter import scatter\n",
    "from openpoints.utils import set_random_seed, save_checkpoint, load_checkpoint, resume_checkpoint, setup_logger_dist, \\\n",
    "    cal_model_parm_nums, Wandb, generate_exp_directory, resume_exp_directory, EasyConfig, dist_utils, find_free_port\n",
    "from openpoints.utils import AverageMeter, ConfusionMatrix, get_mious\n",
    "from openpoints.dataset import build_dataloader_from_cfg, get_features_by_keys, get_class_weights\n",
    "from openpoints.dataset.data_util import voxelize\n",
    "from openpoints.dataset.semantic_kitti.semantickitti import load_label_kitti, load_pc_kitti, remap_lut_read, remap_lut_write, get_semantickitti_file_list\n",
    "from openpoints.transforms import build_transforms_from_cfg\n",
    "from openpoints.optim import build_optimizer_from_cfg\n",
    "from openpoints.scheduler import build_scheduler_from_cfg\n",
    "from openpoints.loss import build_criterion_from_cfg\n",
    "from openpoints.models import build_model_from_cfg\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "def write_to_csv(oa, macc, miou, ious, best_epoch, cfg, write_header=True, area=5):\n",
    "    ious_table = [f'{item:.2f}' for item in ious]\n",
    "    header = ['method', 'Area', 'OA', 'mACC', 'mIoU'] + cfg.classes + ['best_epoch', 'log_path', 'wandb link']\n",
    "    data = [cfg.cfg_basename, str(area), f'{oa:.2f}', f'{macc:.2f}',\n",
    "            f'{miou:.2f}'] + ious_table + [str(best_epoch), cfg.run_dir,\n",
    "                                           wandb.run.get_url() if cfg.wandb.use_wandb else '-']\n",
    "    with open(cfg.csv_path, 'a', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow(header)\n",
    "        writer.writerow(data)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def generate_data_list(cfg):\n",
    "    if 's3dis' in cfg.dataset.common.NAME.lower():\n",
    "        raw_root = os.path.join(cfg.dataset.common.data_root, 'raw')\n",
    "        data_list = sorted(os.listdir(raw_root))\n",
    "        data_list = [os.path.join(raw_root, item) for item in data_list if\n",
    "                     'Area_{}'.format(cfg.dataset.common.test_area) in item]\n",
    "    elif 'scannet' in cfg.dataset.common.NAME.lower():\n",
    "        data_list = glob.glob(os.path.join(cfg.dataset.common.data_root, cfg.dataset.test.split, \"*.pth\"))\n",
    "    elif 'semantickitti' in cfg.dataset.common.NAME.lower():\n",
    "        if cfg.dataset.test.split == 'val':\n",
    "            split_no = 1\n",
    "        else:\n",
    "            split_no = 2\n",
    "        data_list = get_semantickitti_file_list(os.path.join(cfg.dataset.common.data_root, 'sequences'),\n",
    "                                                str(cfg.dataset.test.test_id + 11))[split_no]\n",
    "    else:\n",
    "        raise Exception('{} dataset not supported yet'.format(args.data_name))\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def load_data(data_path, cfg):\n",
    "    label, feat = None, None\n",
    "    if 's3dis' in cfg.dataset.common.NAME.lower():\n",
    "        data = np.load(data_path)  # xyzrgbl, N*7\n",
    "        coord, feat, label = data[:, :3], data[:, 3:6], data[:, 6]\n",
    "        feat = np.clip(feat / 255., 0, 1).astype(np.float32)\n",
    "    elif 'scannet' in cfg.dataset.common.NAME.lower():\n",
    "        data = torch.load(data_path)  # xyzrgbl, N*7\n",
    "        coord, feat = data[0], data[1]\n",
    "        if cfg.dataset.test.split != 'test':\n",
    "            label = data[2]\n",
    "        else:\n",
    "            label = None\n",
    "        feat = np.clip((feat + 1) / 2., 0, 1).astype(np.float32)\n",
    "    elif 'semantickitti' in cfg.dataset.common.NAME.lower():\n",
    "        coord = load_pc_kitti(data_path[0])\n",
    "        if cfg.dataset.test.split != 'test':\n",
    "            label = load_label_kitti(data_path[1], remap_lut_read)\n",
    "    coord -= coord.min(0)\n",
    "\n",
    "    idx_points = []\n",
    "    voxel_idx, reverse_idx_part, reverse_idx_sort = None, None, None\n",
    "    voxel_size = cfg.dataset.common.get('voxel_size', None)\n",
    "\n",
    "    if voxel_size is not None:\n",
    "        # idx_sort: original point indicies sorted by voxel NO.\n",
    "        # voxel_idx: Voxel NO. for the sorted points\n",
    "        idx_sort, voxel_idx, count = voxelize(coord, voxel_size, mode=1)\n",
    "        if cfg.get('test_mode', 'multi_voxel') == 'nearest_neighbor':\n",
    "            idx_select = np.cumsum(np.insert(count, 0, 0)[0:-1]) + np.random.randint(0, count.max(), count.size) % count\n",
    "            idx_part = idx_sort[idx_select]\n",
    "            npoints_subcloud = voxel_idx.max() + 1\n",
    "            idx_shuffle = np.random.permutation(npoints_subcloud)\n",
    "            idx_part = idx_part[idx_shuffle]  # idx_part: randomly sampled points of a voxel\n",
    "            reverse_idx_part = np.argsort(idx_shuffle, axis=0)  # revevers idx_part to sorted\n",
    "            idx_points.append(idx_part)\n",
    "            reverse_idx_sort = np.argsort(idx_sort, axis=0)\n",
    "        else:\n",
    "            for i in range(count.max()):\n",
    "                idx_select = np.cumsum(np.insert(count, 0, 0)[0:-1]) + i % count\n",
    "                idx_part = idx_sort[idx_select]\n",
    "                np.random.shuffle(idx_part)\n",
    "                idx_points.append(idx_part)\n",
    "    else:\n",
    "        idx_points.append(np.arange(label.shape[0]))\n",
    "    return coord, feat, label, idx_points, voxel_idx, reverse_idx_part, reverse_idx_sort\n",
    "\n",
    "\n",
    "def main(gpu, cfg):\n",
    "    if cfg.distributed:\n",
    "        if cfg.mp:\n",
    "            cfg.rank = gpu\n",
    "        dist.init_process_group(backend=cfg.dist_backend,\n",
    "                                init_method=cfg.dist_url,\n",
    "                                world_size=cfg.world_size,\n",
    "                                rank=cfg.rank)\n",
    "        dist.barrier()\n",
    "\n",
    "    # logger\n",
    "    setup_logger_dist(cfg.log_path, cfg.rank, name=cfg.dataset.common.NAME)\n",
    "    if cfg.rank == 0:\n",
    "        Wandb.launch(cfg, cfg.wandb.use_wandb)\n",
    "        writer = SummaryWriter(log_dir=cfg.run_dir) if cfg.is_training else None\n",
    "    else:\n",
    "        writer = None\n",
    "    set_random_seed(cfg.seed + cfg.rank, deterministic=cfg.deterministic)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    logging.info(cfg)\n",
    "\n",
    "    if cfg.model.get('in_channels', None) is None:\n",
    "        cfg.model.in_channels = cfg.model.encoder_args.in_channels\n",
    "    model = build_model_from_cfg(cfg.model).to(cfg.rank)\n",
    "    model_size = cal_model_parm_nums(model)\n",
    "    logging.info(model)\n",
    "    logging.info('Number of params: %.4f M' % (model_size / 1e6))\n",
    "    \n",
    "    model \n",
    "    exit()\n",
    "\n",
    "    if cfg.sync_bn:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        logging.info('Using Synchronized BatchNorm ...')\n",
    "    if cfg.distributed:\n",
    "        torch.cuda.set_device(gpu)\n",
    "        model = nn.parallel.DistributedDataParallel(model.cuda(), device_ids=[cfg.rank], output_device=cfg.rank)\n",
    "        logging.info('Using Distributed Data parallel ...')\n",
    "\n",
    "    # optimizer & scheduler\n",
    "    optimizer = build_optimizer_from_cfg(model, lr=cfg.lr, **cfg.optimizer)\n",
    "    scheduler = build_scheduler_from_cfg(cfg, optimizer)\n",
    "\n",
    "    # build dataset\n",
    "    val_loader = build_dataloader_from_cfg(cfg.get('val_batch_size', cfg.batch_size),\n",
    "                                           cfg.dataset,\n",
    "                                           cfg.dataloader,\n",
    "                                           datatransforms_cfg=cfg.datatransforms,\n",
    "                                           split='val',\n",
    "                                           distributed=cfg.distributed\n",
    "                                           )\n",
    "    logging.info(f\"length of validation dataset: {len(val_loader.dataset)}\")\n",
    "    num_classes = val_loader.dataset.num_classes if hasattr(val_loader.dataset, 'num_classes') else None\n",
    "    if num_classes is not None:\n",
    "        assert cfg.num_classes == num_classes\n",
    "    logging.info(f\"number of classes of the dataset: {num_classes}\")\n",
    "    cfg.classes = val_loader.dataset.classes if hasattr(val_loader.dataset, 'classes') else np.arange(num_classes)\n",
    "    cfg.cmap = np.array(val_loader.dataset.cmap) if hasattr(val_loader.dataset, 'cmap') else None\n",
    "    validate_fn = validate if 'sphere' not in cfg.dataset.common.NAME.lower() else validate_sphere\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    model_module = model.module if hasattr(model, 'module') else model\n",
    "    if cfg.pretrained_path is not None:\n",
    "        if cfg.mode == 'resume':\n",
    "            resume_checkpoint(cfg, model, optimizer, scheduler, pretrained_path=cfg.pretrained_path)\n",
    "        else:\n",
    "            if cfg.mode == 'val':\n",
    "                best_epoch, best_val = load_checkpoint(model, pretrained_path=cfg.pretrained_path)\n",
    "                val_miou, val_macc, val_oa, val_ious, val_accs = validate_fn(model, val_loader, cfg, num_votes=1)\n",
    "                with np.printoptions(precision=2, suppress=True):\n",
    "                    logging.info(\n",
    "                        f'Best ckpt @E{best_epoch},  val_oa , val_macc, val_miou: {val_oa:.2f} {val_macc:.2f} {val_miou:.2f}, '\n",
    "                        f'\\niou per cls is: {val_ious}')\n",
    "                return val_miou\n",
    "            elif cfg.mode == 'test':\n",
    "                best_epoch, best_val = load_checkpoint(model, pretrained_path=cfg.pretrained_path)\n",
    "                data_list = generate_data_list(cfg)\n",
    "                logging.info(f\"length of test dataset: {len(data_list)}\")\n",
    "                test_miou, test_macc, test_oa, test_ious, test_accs, _ = test(model, data_list, cfg)\n",
    "\n",
    "                if test_miou is not None:\n",
    "                    with np.printoptions(precision=2, suppress=True):\n",
    "                        logging.info(\n",
    "                            f'Best ckpt @E{best_epoch},  test_oa , test_macc, test_miou: {test_oa:.2f} {test_macc:.2f} {test_miou:.2f}, '\n",
    "                            f'\\niou per cls is: {test_ious}')\n",
    "                    cfg.csv_path = os.path.join(cfg.run_dir, cfg.run_name + '_test.csv')\n",
    "                    write_to_csv(test_oa, test_macc, test_miou, test_ious, best_epoch, cfg)\n",
    "                return test_miou\n",
    "\n",
    "            elif 'encoder' in cfg.mode:\n",
    "                logging.info(f'Finetuning from {cfg.pretrained_path}')\n",
    "                load_checkpoint(model_module.encoder, cfg.pretrained_path, cfg.get('pretrained_module', None))\n",
    "            else:\n",
    "                logging.info(f'Finetuning from {cfg.pretrained_path}')\n",
    "                load_checkpoint(model, cfg.pretrained_path, cfg.get('pretrained_module', None))\n",
    "    else:\n",
    "        logging.info('Training from scratch')\n",
    "\n",
    "    if 'freeze_blocks' in cfg.mode:\n",
    "        for p in model_module.encoder.blocks.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    train_loader = build_dataloader_from_cfg(cfg.batch_size,\n",
    "                                             cfg.dataset,\n",
    "                                             cfg.dataloader,\n",
    "                                             datatransforms_cfg=cfg.datatransforms,\n",
    "                                             split='train',\n",
    "                                             distributed=cfg.distributed,\n",
    "                                             )\n",
    "    logging.info(f\"length of training dataset: {len(train_loader.dataset)}\")\n",
    "    logging.info(f\"number of training batch: {len(train_loader)}\")\n",
    "\n",
    "    cfg.criterion_args.weight = None\n",
    "    if cfg.get('cls_weighed_loss', False):\n",
    "        if hasattr(train_loader.dataset, 'num_per_class'):\n",
    "            cfg.criterion_args.weight = get_class_weights(train_loader.dataset.num_per_class, normalize=True)\n",
    "        else:\n",
    "            logging.info('`num_per_class` attribute is not founded in dataset')\n",
    "    criterion = build_criterion_from_cfg(cfg.criterion_args).cuda()\n",
    "\n",
    "    # ===> start training\n",
    "    if cfg.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    val_miou, val_macc, val_oa, val_ious, val_accs = 0., 0., 0., [], []\n",
    "    best_val, macc_when_best, oa_when_best, ious_when_best, best_epoch = 0., 0., 0., [], 0\n",
    "    for epoch in range(cfg.start_epoch, cfg.epochs + 1):\n",
    "        if cfg.distributed:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        if hasattr(train_loader.dataset, 'epoch'):  # some dataset sets the dataset length as a fixed steps.\n",
    "            train_loader.dataset.epoch = epoch - 1\n",
    "        # train_loss, train_miou, train_macc, train_oa, _, _ = \\\n",
    "        #     train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, epoch, cfg)\n",
    "\n",
    "        is_best = False\n",
    "        if epoch % cfg.val_freq == 0:\n",
    "            val_miou, val_macc, val_oa, val_ious, val_accs = validate_fn(model, val_loader, cfg)\n",
    "            if val_miou > best_val:\n",
    "                is_best = True\n",
    "                best_val = val_miou\n",
    "                macc_when_best = val_macc\n",
    "                oa_when_best = val_oa\n",
    "                ious_when_best = val_ious\n",
    "                best_epoch = epoch\n",
    "                with np.printoptions(precision=2, suppress=True):\n",
    "                    logging.info(\n",
    "                        f'Find a better ckpt @E{epoch}, val_miou {val_miou:.2f} val_macc {macc_when_best:.2f}, val_oa {oa_when_best:.2f}'\n",
    "                        f'\\nmious: {val_ious}')\n",
    "\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        logging.info(f'Epoch {epoch} LR {lr:.6f} '\n",
    "                     f'train_miou {train_miou:.2f}, val_miou {val_miou:.2f}, best val miou {best_val:.2f}')\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('best_val', best_val, epoch)\n",
    "            writer.add_scalar('val_miou', val_miou, epoch)\n",
    "            writer.add_scalar('macc_when_best', macc_when_best, epoch)\n",
    "            writer.add_scalar('oa_when_best', oa_when_best, epoch)\n",
    "            writer.add_scalar('val_macc', val_macc, epoch)\n",
    "            writer.add_scalar('val_oa', val_oa, epoch)\n",
    "            writer.add_scalar('train_loss', train_loss, epoch)\n",
    "            writer.add_scalar('train_miou', train_miou, epoch)\n",
    "            writer.add_scalar('train_macc', train_macc, epoch)\n",
    "            writer.add_scalar('lr', lr, epoch)\n",
    "\n",
    "        if cfg.sched_on_epoch:\n",
    "            scheduler.step(epoch)\n",
    "        if cfg.rank == 0:\n",
    "            save_checkpoint(cfg, model, epoch, optimizer, scheduler,\n",
    "                            additioanl_dict={'best_val': best_val},\n",
    "                            is_best=is_best\n",
    "                            )\n",
    "            is_best = False\n",
    "    # do not save file to wandb to save wandb space\n",
    "    # if writer is not None:\n",
    "    #     Wandb.add_file(os.path.join(cfg.ckpt_dir, f'{cfg.run_name}_ckpt_best.pth'))\n",
    "    # Wandb.add_file(os.path.join(cfg.ckpt_dir, f'{cfg.logname}_ckpt_latest.pth'))\n",
    "\n",
    "    # validate\n",
    "    with np.printoptions(precision=2, suppress=True):\n",
    "        logging.info(\n",
    "            f'Best ckpt @E{best_epoch},  val_oa {oa_when_best:.2f}, val_macc {macc_when_best:.2f}, val_miou {best_val:.2f}, '\n",
    "            f'\\niou per cls is: {ious_when_best}')\n",
    "\n",
    "    if cfg.world_size < 2:  # do not support multi gpu testing\n",
    "        # test\n",
    "        load_checkpoint(model, pretrained_path=os.path.join(cfg.ckpt_dir, f'{cfg.run_name}_ckpt_best.pth'))\n",
    "        cfg.csv_path = os.path.join(cfg.run_dir, cfg.run_name + f'.csv')\n",
    "        if 'sphere' in cfg.dataset.common.NAME.lower():\n",
    "            test_miou, test_macc, test_oa, test_ious, test_accs = validate_sphere(model, val_loader, cfg)\n",
    "        else:\n",
    "            data_list = generate_data_list(cfg)\n",
    "            test_miou, test_macc, test_oa, test_ious, test_accs, _ = test(model, data_list, cfg)\n",
    "        with np.printoptions(precision=2, suppress=True):\n",
    "            logging.info(\n",
    "                f'Best ckpt @E{best_epoch},  test_oa {test_oa:.2f}, test_macc {test_macc:.2f}, test_miou {test_miou:.2f}, '\n",
    "                f'\\niou per cls is: {test_ious}')\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('test_miou', test_miou, epoch)\n",
    "            writer.add_scalar('test_macc', test_macc, epoch)\n",
    "            writer.add_scalar('test_oa', test_oa, epoch)\n",
    "        write_to_csv(test_oa, test_macc, test_miou, test_ious, best_epoch, cfg, write_header=True)\n",
    "        logging.info(f'save results in {cfg.csv_path}')\n",
    "        if cfg.use_voting:\n",
    "            load_checkpoint(model, pretrained_path=os.path.join(cfg.ckpt_dir, f'{cfg.run_name}_ckpt_best.pth'))\n",
    "            set_random_seed(cfg.seed)\n",
    "            val_miou, val_macc, val_oa, val_ious, val_accs = validate_fn(model, val_loader, cfg, num_votes=20,\n",
    "                                                                         data_transform=data_transform)\n",
    "            if writer is not None:\n",
    "                writer.add_scalar('val_miou20', val_miou, cfg.epochs + 50)\n",
    "\n",
    "            ious_table = [f'{item:.2f}' for item in val_ious]\n",
    "            data = [cfg.cfg_basename, 'True', f'{val_oa:.2f}', f'{val_macc:.2f}', f'{val_miou:.2f}'] + ious_table + [\n",
    "                str(best_epoch), cfg.run_dir]\n",
    "            with open(cfg.csv_path, 'w', encoding='UT8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(data)\n",
    "    else:\n",
    "        logging.warning(\n",
    "            'Testing using multiple GPUs is not allowed for now. Running testing after this training is required.')\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    # dist.destroy_process_group() # comment this line due to https://github.com/guochengqian/PointNeXt/issues/95\n",
    "    wandb.finish(exit_code=True)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, epoch, cfg):\n",
    "    loss_meter = AverageMeter()\n",
    "    cm = ConfusionMatrix(num_classes=cfg.num_classes, ignore_index=cfg.ignore_index)\n",
    "    model.train()  # set model to training mode\n",
    "    pbar = tqdm(enumerate(train_loader), total=train_loader.__len__())\n",
    "    num_iter = 0\n",
    "    for idx, data in pbar:\n",
    "        keys = data.keys() if callable(data.keys) else data.keys\n",
    "        for key in keys:\n",
    "            data[key] = data[key].cuda(non_blocking=True)\n",
    "        num_iter += 1\n",
    "        target = data['y'].squeeze(-1)\n",
    "        \"\"\" debug\n",
    "        from openpoints.dataset import vis_points\n",
    "        vis_points(data['pos'].cpu().numpy()[0], labels=data['y'].cpu().numpy()[0])\n",
    "        vis_points(data['pos'].cpu().numpy()[0], data['x'][0, :3, :].transpose(1, 0))\n",
    "        end of debug \"\"\"\n",
    "        data['x'] = get_features_by_keys(data, cfg.feature_keys)\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, target) if 'mask' not in cfg.criterion_args.NAME.lower() \\\n",
    "                else criterion(logits, target, data['mask'])\n",
    "\n",
    "        if cfg.use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        # optimize\n",
    "        if num_iter == cfg.step_per_update:\n",
    "            if cfg.get('grad_norm_clip') is not None and cfg.grad_norm_clip > 0.:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_norm_clip, norm_type=2)\n",
    "            num_iter = 0\n",
    "\n",
    "            if cfg.use_amp:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if not cfg.sched_on_epoch:\n",
    "                scheduler.step(epoch)\n",
    "\n",
    "        # update confusion matrix\n",
    "        cm.update(logits.argmax(dim=1), target)\n",
    "        loss_meter.update(loss.item())\n",
    "\n",
    "        if idx % cfg.print_freq:\n",
    "            pbar.set_description(f\"Train Epoch [{epoch}/{cfg.epochs}] \"\n",
    "                                 f\"Loss {loss_meter.val:.3f} Acc {cm.overall_accuray:.2f}\")\n",
    "    miou, macc, oa, ious, accs = cm.all_metrics()\n",
    "    return loss_meter.avg, miou, macc, oa, ious, accs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, cfg, num_votes=1, data_transform=None):\n",
    "    model.eval()  # set model to eval mode\n",
    "    cm = ConfusionMatrix(num_classes=cfg.num_classes, ignore_index=cfg.ignore_index)\n",
    "    pbar = tqdm(enumerate(val_loader), total=val_loader.__len__(), desc='Val')\n",
    "    for idx, data in pbar:\n",
    "        \n",
    "        if idx <= 25:\n",
    "            continue\n",
    "        keys = data.keys() if callable(data.keys) else data.keys\n",
    "        for key in keys:\n",
    "            data[key] = data[key].cuda(non_blocking=True)\n",
    "        target = data['y'].squeeze(-1)\n",
    "        data['x'] = get_features_by_keys(data, cfg.feature_keys)\n",
    "        import jhutil; jhutil.jhprint(3333, data)\n",
    "        logits = model(data)\n",
    "        import jhutil; jhutil.jhprint(4444, )\n",
    "        if 'mask' not in cfg.criterion_args.NAME or cfg.get('use_maks', False):\n",
    "            cm.update(logits.argmax(dim=1), target)\n",
    "        else:\n",
    "            mask = data['mask'].bool()\n",
    "            cm.update(logits.argmax(dim=1)[mask], target[mask])\n",
    "        import jhutil; jhutil.jhprint(5555, )\n",
    "\n",
    "        \"\"\"visualization in debug mode\n",
    "        from openpoints.dataset.vis3d import vis_points, vis_multi_points\n",
    "        coord = data['pos'].cpu().numpy()[0]\n",
    "        pred = logits.argmax(dim=1)[0].cpu().numpy()\n",
    "        label = target[0].cpu().numpy()\n",
    "        if cfg.ignore_index is not None:\n",
    "            if (label == cfg.ignore_index).sum() > 0:\n",
    "                pred[label == cfg.ignore_index] = cfg.num_classes\n",
    "                label[label == cfg.ignore_index] = cfg.num_classes\n",
    "        vis_multi_points([coord, coord], labels=[label, pred])\n",
    "        \"\"\"\n",
    "        # tp, union, count = cm.tp, cm.union, cm.count\n",
    "        # if cfg.distributed:\n",
    "        #     dist.all_reduce(tp), dist.all_reduce(union), dist.all_reduce(count)\n",
    "        # miou, macc, oa, ious, accs = get_mious(tp, union, count)\n",
    "        # with np.printoptions(precision=2, suppress=True):\n",
    "        #     logging.info(f'{idx}-th cloud,  test_oa , test_macc, test_miou: {oa:.2f} {macc:.2f} {miou:.2f}, '\n",
    "        #                 f'\\niou per cls is: {ious}')\n",
    "\n",
    "    tp, union, count = cm.tp, cm.union, cm.count\n",
    "    if cfg.distributed:\n",
    "        dist.all_reduce(tp), dist.all_reduce(union), dist.all_reduce(count)\n",
    "    miou, macc, oa, ious, accs = get_mious(tp, union, count)\n",
    "    return miou, macc, oa, ious, accs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_sphere(model, val_loader, cfg, num_votes=1, data_transform=None):\n",
    "    \"\"\"\n",
    "    validation for sphere sampled input points with mask.\n",
    "    in this case, between different batches, there are overlapped points.\n",
    "    thus, one point can be evaluated multiple times.\n",
    "    In this validate_mask, we will avg the logits.\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to eval mode\n",
    "    cm = ConfusionMatrix(num_classes=cfg.num_classes, ignore_index=cfg.ignore_index)\n",
    "    if cfg.get('visualize', False):\n",
    "        from openpoints.dataset.vis3d import write_obj\n",
    "        cfg.vis_dir = os.path.join(cfg.run_dir, 'visualization')\n",
    "        os.makedirs(cfg.vis_dir, exist_ok=True)\n",
    "        cfg.cmap = cfg.cmap.astype(np.float32) / 255.\n",
    "\n",
    "    pbar = tqdm(enumerate(val_loader), total=val_loader.__len__())\n",
    "    all_logits, idx_points = [], []\n",
    "    for idx, data in pbar:\n",
    "        for key in data.keys():\n",
    "            data[key] = data[key].cuda(non_blocking=True)\n",
    "        data['x'] = get_features_by_keys(data, cfg.feature_keys)\n",
    "        logits = model(data)\n",
    "        all_logits.append(logits)\n",
    "        idx_points.append(data['input_inds'])\n",
    "    all_logits = torch.cat(all_logits, dim=0).transpose(1, 2).reshape(-1, cfg.num_classes)\n",
    "    idx_points = torch.cat(idx_points, dim=0).flatten()\n",
    "\n",
    "    if cfg.distributed:\n",
    "        dist.all_reduce(all_logits), dist.all_reduce(idx_points)\n",
    "\n",
    "    # average overlapped predictions to subsampled points\n",
    "    all_logits = scatter(all_logits, idx_points, dim=0, reduce='mean')\n",
    "\n",
    "    # now, project the original points to the subsampled points\n",
    "    # these two targets would be very similar but not the same\n",
    "    # val_points_targets = all_targets[val_points_projections]\n",
    "    # torch.allclose(val_points_labels, val_points_targets)\n",
    "    all_logits = all_logits.argmax(dim=1)\n",
    "    val_points_labels = torch.from_numpy(val_loader.dataset.clouds_points_labels[0]).squeeze(-1).to(all_logits.device)\n",
    "    val_points_projections = torch.from_numpy(val_loader.dataset.projections[0]).to(all_logits.device).long()\n",
    "    val_points_preds = all_logits[val_points_projections]\n",
    "\n",
    "    del all_logits, idx_points\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cm.update(val_points_preds, val_points_labels)\n",
    "    miou, macc, oa, ious, accs = cm.all_metrics()\n",
    "\n",
    "    if cfg.get('visualize', False):\n",
    "        dataset_name = cfg.dataset.common.NAME.lower()\n",
    "        coord = val_loader.dataset.clouds_points[0]\n",
    "        colors = val_loader.dataset.clouds_points_colors[0].astype(np.float32)\n",
    "        gt = val_points_labels.cpu().numpy().squeeze()\n",
    "        pred = val_points_preds.cpu().numpy().squeeze()\n",
    "        gt = cfg.cmap[gt, :]\n",
    "        pred = cfg.cmap[pred, :]\n",
    "        # output pred labels\n",
    "        # save per room\n",
    "        rooms = val_loader.dataset.clouds_rooms[0]\n",
    "\n",
    "        for idx in tqdm(range(len(rooms) - 1), desc='save visualization'):\n",
    "            start_idx, end_idx = rooms[idx], rooms[idx + 1]\n",
    "            write_obj(coord[start_idx:end_idx], colors[start_idx:end_idx],\n",
    "                      os.path.join(cfg.vis_dir, f'input-{dataset_name}-{idx}.obj'))\n",
    "            # output ground truth labels\n",
    "            write_obj(coord[start_idx:end_idx], gt[start_idx:end_idx],\n",
    "                      os.path.join(cfg.vis_dir, f'gt-{dataset_name}-{idx}.obj'))\n",
    "            # output pred labels\n",
    "            write_obj(coord[start_idx:end_idx], pred[start_idx:end_idx],\n",
    "                      os.path.join(cfg.vis_dir, f'{cfg.cfg_basename}-{dataset_name}-{idx}.obj'))\n",
    "    return miou, macc, oa, ious, accs\n",
    "\n",
    "\n",
    "# TODO: multi gpu support. Warp to a dataloader.\n",
    "@torch.no_grad()\n",
    "def test(model, data_list, cfg, num_votes=1):\n",
    "    \"\"\"using a part of original point cloud as input to save memory.\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        test_loader (_type_): _description_\n",
    "        cfg (_type_): _description_\n",
    "        num_votes (int, optional): _description_. Defaults to 1.\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to eval mode\n",
    "    all_cm = ConfusionMatrix(num_classes=cfg.num_classes, ignore_index=cfg.ignore_index)\n",
    "    set_random_seed(0)\n",
    "    cfg.visualize = cfg.get('visualize', False)\n",
    "    if cfg.visualize:\n",
    "        from openpoints.dataset.vis3d import write_obj\n",
    "        cfg.vis_dir = os.path.join(cfg.run_dir, 'visualization')\n",
    "        os.makedirs(cfg.vis_dir, exist_ok=True)\n",
    "        cfg.cmap = cfg.cmap.astype(np.float32) / 255.\n",
    "\n",
    "    # data\n",
    "    trans_split = 'val' if cfg.datatransforms.get('test', None) is None else 'test'\n",
    "    pipe_transform = build_transforms_from_cfg(trans_split, cfg.datatransforms)\n",
    "\n",
    "    dataset_name = cfg.dataset.common.NAME.lower()\n",
    "    len_data = len(data_list)\n",
    "\n",
    "    cfg.save_path = cfg.get('save_path', f'results/{cfg.task_name}/{cfg.dataset.test.split}/{cfg.cfg_basename}')\n",
    "    if 'semantickitti' in cfg.dataset.common.NAME.lower():\n",
    "        cfg.save_path = os.path.join(cfg.save_path, str(cfg.dataset.test.test_id + 11), 'predictions')\n",
    "    os.makedirs(cfg.save_path, exist_ok=True)\n",
    "\n",
    "    gravity_dim = cfg.datatransforms.kwargs.gravity_dim\n",
    "    nearest_neighbor = cfg.get('test_mode', 'multi_voxel') == 'nearest_neighbor'\n",
    "    for cloud_idx, data_path in enumerate(data_list):\n",
    "        logging.info(f'Test [{cloud_idx}]/[{len_data}] cloud')\n",
    "        cm = ConfusionMatrix(num_classes=cfg.num_classes, ignore_index=cfg.ignore_index)\n",
    "        all_logits = []\n",
    "        coord, feat, label, idx_points, voxel_idx, reverse_idx_part, reverse_idx = load_data(data_path, cfg)\n",
    "        if label is not None:\n",
    "            label = torch.from_numpy(label.astype(np.int).squeeze()).cuda(non_blocking=True)\n",
    "\n",
    "        len_part = len(idx_points)\n",
    "        nearest_neighbor = len_part == 1\n",
    "        pbar = tqdm(range(len(idx_points)))\n",
    "        for idx_subcloud in pbar:\n",
    "            pbar.set_description(f\"Test on {cloud_idx}-th cloud [{idx_subcloud}]/[{len_part}]]\")\n",
    "            if not (nearest_neighbor and idx_subcloud > 0):\n",
    "                idx_part = idx_points[idx_subcloud]\n",
    "                coord_part = coord[idx_part]\n",
    "                coord_part -= coord_part.min(0)\n",
    "\n",
    "                feat_part = feat[idx_part] if feat is not None else None\n",
    "                data = {'pos': coord_part}\n",
    "                if feat_part is not None:\n",
    "                    data['x'] = feat_part\n",
    "                if pipe_transform is not None:\n",
    "                    data = pipe_transform(data)\n",
    "                if 'heights' in cfg.feature_keys and 'heights' not in data.keys():\n",
    "                    if 'semantickitti' in cfg.dataset.common.NAME.lower():\n",
    "                        data['heights'] = torch.from_numpy(\n",
    "                            (coord_part[:, gravity_dim:gravity_dim + 1] - coord_part[:, gravity_dim:gravity_dim + 1].min()).astype(np.float32)).unsqueeze(0)\n",
    "                    else:\n",
    "                        data['heights'] = torch.from_numpy(\n",
    "                            coord_part[:, gravity_dim:gravity_dim + 1].astype(np.float32)).unsqueeze(0)\n",
    "                if not cfg.dataset.common.get('variable', False):\n",
    "                    if 'x' in data.keys():\n",
    "                        data['x'] = data['x'].unsqueeze(0)\n",
    "                    data['pos'] = data['pos'].unsqueeze(0)\n",
    "                else:\n",
    "                    data['o'] = torch.IntTensor([len(coord)])\n",
    "                    data['batch'] = torch.LongTensor([0] * len(coord))\n",
    "\n",
    "                for key in data.keys():\n",
    "                    data[key] = data[key].cuda(non_blocking=True)\n",
    "                data['x'] = get_features_by_keys(data, cfg.feature_keys)\n",
    "                logits = model(data)\n",
    "                \"\"\"visualization in debug mode. !!! visulization is not correct, should remove ignored idx.\n",
    "                from openpoints.dataset.vis3d import vis_points, vis_multi_points\n",
    "                vis_multi_points([coord, coord_part], labels=[label.cpu().numpy(), logits.argmax(dim=1).squeeze().cpu().numpy()])\n",
    "                \"\"\"\n",
    "\n",
    "            all_logits.append(logits)\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "        if not cfg.dataset.common.get('variable', False):\n",
    "            all_logits = all_logits.transpose(1, 2).reshape(-1, cfg.num_classes)\n",
    "\n",
    "        if not nearest_neighbor:\n",
    "            # average merge overlapped multi voxels logits to original point set\n",
    "            idx_points = torch.from_numpy(np.hstack(idx_points)).cuda(non_blocking=True)\n",
    "            all_logits = scatter(all_logits, idx_points, dim=0, reduce='mean')\n",
    "        else:\n",
    "            # interpolate logits by nearest neighbor\n",
    "            all_logits = all_logits[reverse_idx_part][voxel_idx][reverse_idx]\n",
    "        pred = all_logits.argmax(dim=1)\n",
    "        if label is not None:\n",
    "            cm.update(pred, label)\n",
    "        \"\"\"visualization in debug mode\n",
    "        from openpoints.dataset.vis3d import vis_points, vis_multi_points\n",
    "        vis_multi_points([coord, coord], labels=[label.cpu().numpy(), all_logits.argmax(dim=1).squeeze().cpu().numpy()])\n",
    "        \"\"\"\n",
    "        if cfg.visualize:\n",
    "            gt = label.cpu().numpy().squeeze() if label is not None else None\n",
    "            pred = pred.cpu().numpy().squeeze()\n",
    "            gt = cfg.cmap[gt, :] if gt is not None else None\n",
    "            pred = cfg.cmap[pred, :]\n",
    "            # output pred labels\n",
    "            if 's3dis' in dataset_name:\n",
    "                file_name = f'{dataset_name}-Area{cfg.dataset.common.test_area}-{cloud_idx}'\n",
    "            else:\n",
    "                file_name = f'{dataset_name}-{cloud_idx}'\n",
    "\n",
    "            write_obj(coord, feat,\n",
    "                      os.path.join(cfg.vis_dir, f'input-{file_name}.obj'))\n",
    "            # output ground truth labels\n",
    "            if gt is not None:\n",
    "                write_obj(coord, gt,\n",
    "                          os.path.join(cfg.vis_dir, f'gt-{file_name}.obj'))\n",
    "            # output pred labels\n",
    "            write_obj(coord, pred,\n",
    "                      os.path.join(cfg.vis_dir, f'{cfg.cfg_basename}-{file_name}.obj'))\n",
    "\n",
    "        if cfg.get('save_pred', False):\n",
    "            if 'semantickitti' in cfg.dataset.common.NAME.lower():\n",
    "                pred = pred + 1\n",
    "                pred = pred.cpu().numpy().squeeze()\n",
    "                pred = pred.astype(np.uint32)\n",
    "                upper_half = pred >> 16  # get upper half for instances\n",
    "                lower_half = pred & 0xFFFF  # get lower half for semantics (lower_half.shape) (100k+, )\n",
    "                lower_half = remap_lut_write[lower_half]  # do the remapping of semantics\n",
    "                pred = (upper_half << 16) + lower_half  # reconstruct full label\n",
    "                pred = pred.astype(np.uint32)\n",
    "                frame_id = data_path[0].split('/')[-1][:-4]\n",
    "                store_path = os.path.join(cfg.save_path, frame_id + '.label')\n",
    "                pred.tofile(store_path)\n",
    "            elif 'scannet' in cfg.dataset.common.NAME.lower():\n",
    "                pred = pred.cpu().numpy().squeeze()\n",
    "                label_int_mapping = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10,\n",
    "                                     10: 11, 11: 12, 12: 14, 13: 16, 14: 24, 15: 28, 16: 33, 17: 34, 18: 36, 19: 39}\n",
    "                pred = np.vectorize(label_int_mapping.get)(pred)\n",
    "                save_file_name = data_path.split('/')[-1].split('_')\n",
    "                save_file_name = save_file_name[0] + '_' + save_file_name[1] + '.txt'\n",
    "                save_file_name = os.path.join(cfg.save_path, save_file_name)\n",
    "                np.savetxt(save_file_name, pred, fmt=\"%d\")\n",
    "\n",
    "        if label is not None:\n",
    "            tp, union, count = cm.tp, cm.union, cm.count\n",
    "            miou, macc, oa, ious, accs = get_mious(tp, union, count)\n",
    "            with np.printoptions(precision=2, suppress=True):\n",
    "                logging.info(\n",
    "                    f'[{cloud_idx}]/[{len_data}] cloud,  test_oa , test_macc, test_miou: {oa:.2f} {macc:.2f} {miou:.2f}, '\n",
    "                    f'\\niou per cls is: {ious}')\n",
    "            all_cm.value += cm.value\n",
    "\n",
    "    if 'scannet' in cfg.dataset.common.NAME.lower():\n",
    "        logging.info(f\" Please select and zip all the files (DON'T INCLUDE THE FOLDER) in {cfg.save_path} and submit it to\"\n",
    "                     f\" Scannet Benchmark https://kaldir.vc.in.tum.de/scannet_benchmark/. \")\n",
    "\n",
    "    if label is not None:\n",
    "        tp, union, count = all_cm.tp, all_cm.union, all_cm.count\n",
    "        if cfg.distributed:\n",
    "            dist.all_reduce(tp), dist.all_reduce(union), dist.all_reduce(count)\n",
    "        miou, macc, oa, ious, accs = get_mious(tp, union, count)\n",
    "        return miou, macc, oa, ious, accs, all_cm\n",
    "    else:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser('Scene segmentation training/testing')\n",
    "    parser.add_argument('--cfg', type=str, required=True, help='config file')\n",
    "    parser.add_argument('--profile', action='store_true', default=False, help='set to True to profile speed')\n",
    "    parser.add_argument('--debug', action='store_true', default=False, help='set to True to profile speed')\n",
    "    args, opts = parser.parse_known_args()\n",
    "    cfg = EasyConfig()\n",
    "    cfg.load(args.cfg, recursive=True)\n",
    "    if args.debug:\n",
    "        cfg[\"dataset\"][\"common\"][\"overfit\"] = 32\n",
    "    \n",
    "    cfg.update(opts)  # overwrite the default arguments in yml\n",
    "\n",
    "    if cfg.seed is None:\n",
    "        cfg.seed = np.random.randint(1, 10000)\n",
    "\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    cfg.rank, cfg.world_size, cfg.distributed, cfg.mp = dist_utils.get_dist_info(cfg)\n",
    "    cfg.sync_bn = cfg.world_size > 1\n",
    "\n",
    "    # init log dir\n",
    "    cfg.task_name = args.cfg.split('.')[-2].split('/')[-2]  # task/dataset name, \\eg s3dis, modelnet40_cls\n",
    "    cfg.cfg_basename = args.cfg.split('.')[-2].split('/')[-1]  # cfg_basename, \\eg pointnext-xl\n",
    "    tags = [\n",
    "        cfg.task_name,  # task name (the folder of name under ./cfgs\n",
    "        cfg.mode,\n",
    "        cfg.cfg_basename,  # cfg file name\n",
    "        f'ngpus{cfg.world_size}',\n",
    "        f'seed{cfg.seed}',\n",
    "    ]\n",
    "    opt_list = []  # for checking experiment configs from logging file\n",
    "    for i, opt in enumerate(opts):\n",
    "        if 'rank' not in opt and 'dir' not in opt and 'root' not in opt and 'pretrain' not in opt and 'path' not in opt and 'wandb' not in opt and '/' not in opt:\n",
    "            opt_list.append(opt)\n",
    "    cfg.root_dir = os.path.join(cfg.root_dir, cfg.task_name)\n",
    "    cfg.opts = '-'.join(opt_list)\n",
    "\n",
    "    cfg.is_training = cfg.mode not in ['test', 'testing', 'val', 'eval', 'evaluation']\n",
    "    if cfg.mode in ['resume', 'val', 'test']:\n",
    "        resume_exp_directory(cfg, pretrained_path=cfg.pretrained_path)\n",
    "        cfg.wandb.tags = [cfg.mode]\n",
    "    else:\n",
    "        generate_exp_directory(cfg, tags, additional_id=os.environ.get('MASTER_PORT', None))\n",
    "        cfg.wandb.tags = tags\n",
    "    os.environ[\"JOB_LOG_DIR\"] = cfg.log_dir\n",
    "    cfg_path = os.path.join(cfg.run_dir, \"cfg.yaml\")\n",
    "    with open(cfg_path, 'w') as f:\n",
    "        yaml.dump(cfg, f, indent=2)\n",
    "        os.system('cp %s %s' % (args.cfg, cfg.run_dir))\n",
    "    cfg.cfg_path = cfg_path\n",
    "\n",
    "    # wandb config\n",
    "    cfg.wandb.name = cfg.run_name\n",
    "\n",
    "    # multi processing.\n",
    "    if cfg.mp:\n",
    "        port = find_free_port()\n",
    "        cfg.dist_url = f\"tcp://localhost:{port}\"\n",
    "        print('using mp spawn for distributed training')\n",
    "        mp.spawn(main, nprocs=cfg.world_size, args=(cfg,))\n",
    "    else:\n",
    "        main(0, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41m1111  \"300\" \"300\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "n_broken_sample = 150\n",
    "n_skin_sample = 150\n",
    "while n_broken_sample + n_skin_sample < 512:\n",
    "    n_broken_sample *= 2\n",
    "    n_skin_sample *= 2\n",
    "\n",
    "import jhutil; jhutil.jhprint(1111, n_broken_sample, n_skin_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "from part_assembly.stage4 import matrix_reduction\n",
    "\n",
    "# Generate a random matrix for demonstration\n",
    "matrix = np.random.rand(10, 10)\n",
    "matrix = torch.Tensor(matrix)\n",
    "\n",
    "# Plot the matrix as a heatmap\n",
    "plt.imshow(matrix, cmap='hot', interpolation='nearest')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "new_matrix = matrix_reduction(matrix, 4, 5)\n",
    "\n",
    "# Plot the matrix as a heatmap\n",
    "plt.imshow(new_matrix, cmap='hot', interpolation='nearest')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [2, 2],\n",
      "        [2, 1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_687/2408107736.py:8: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  indices = torch.stack((indices // similarity_matrix.size(1), indices % similarity_matrix.size(1)), dim=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def top_k_indices(similarity_matrix, k):\n",
    "    # Flatten the matrix and get the values and indices of top-k elements\n",
    "    values, indices = torch.topk(similarity_matrix.view(-1), k)\n",
    "    \n",
    "    # Convert to 2D indices\n",
    "    indices = torch.stack((indices // similarity_matrix.size(1), indices % similarity_matrix.size(1)), dim=1)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "similarity_matrix = torch.Tensor([[1, 2, 10], [4, 5, 6], [7, 8, 9]])\n",
    "print(top_k_indices(similarity_matrix, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_operation(v1, v2):\n",
    "    # Assuming a generic binary operation\n",
    "    if v1 == 1 and v2 == 2:\n",
    "        return 10000\n",
    "    if v1 == 3 and v2 == 4:\n",
    "        return 10000\n",
    "    \n",
    "    return v1 * v2 / 2  # This is a placeholder. Please replace it with the actual operation.\n",
    "\n",
    "class Node:\n",
    "    id_counter = 0\n",
    "\n",
    "    def __init__(self, val, elements, history):\n",
    "        self.id = Node.id_counter\n",
    "        Node.id_counter += 1\n",
    "        self.val = val\n",
    "        self.elements = elements\n",
    "        self.history = history\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.id < other.id\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(1, 3)\n",
      "(1, 4)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Sample data\n",
    "numbers = [1, 2, 3, 4]\n",
    "length = 2\n",
    "\n",
    "# Generate combinations\n",
    "combinations_list = list(combinations(numbers, length))\n",
    "\n",
    "# Print the combinations\n",
    "for combination in combinations_list:\n",
    "    print(combination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "def bfs(v):\n",
    "    # Create initial nodes\n",
    "    nodes = [Node(0, frozenset([x]), [(x,)]) for x in v]\n",
    "    visited = set(nodes)\n",
    "    max_value = max(node.val for node in nodes)\n",
    "    max_history = []\n",
    "\n",
    "    # Priority queue with elements in form (-value, node)\n",
    "    # We use negative value because heapq in python is a min heap\n",
    "    priority_queue = [(-node.val, node) for node in nodes]\n",
    "    heapq.heapify(priority_queue)\n",
    "\n",
    "    while priority_queue:\n",
    "        # Pop the node with the highest value\n",
    "        neg_value, node = heapq.heappop(priority_queue)\n",
    "        value = -neg_value\n",
    "\n",
    "        # Check if we have used all values\n",
    "        if len(node.elements) == len(v):\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                max_history = node.history\n",
    "            continue\n",
    "\n",
    "        # Create new nodes by combining current node with remaining nodes\n",
    "        remaining_nodes = [n for n in nodes if n.elements.isdisjoint(node.elements)]\n",
    "        for remaining_node in remaining_nodes:\n",
    "            new_val = at_operation(node.val, remaining_node.val)\n",
    "            new_elements = node.elements.union(remaining_node.elements)\n",
    "            new_history = node.history + [(node.val, remaining_node.val, new_val)]\n",
    "            new_node = Node(new_val, new_elements, new_history)\n",
    "\n",
    "            if new_node not in visited:\n",
    "                visited.add(new_node)\n",
    "                heapq.heappush(priority_queue, (-new_val, new_node))\n",
    "\n",
    "    return max_value, max_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation history:\n",
      "1 @ 2 = 10000\n",
      "10000 @ 5 = 25000.0\n",
      "25000.0 @ 4 = 50000.0\n",
      "50000.0 @ 3 = 75000.0\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "v = [1, 2, 3, 4, 5]\n",
    "max_value, max_history = bfs(v)\n",
    "print(\"Operation history:\")\n",
    "for op in max_history[1:]:\n",
    "    print(f\"{op[0]} @ {op[1]} = {op[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointnext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
